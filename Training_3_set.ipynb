{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Import libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from torch import backends\n",
    "from beautifultable import BeautifulTable\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##SETTINGS\n",
    "doTrain = True\n",
    "doEval = True\n",
    "\n",
    "nfold = 5 #number of folds to train\n",
    "fold_offset = 1\n",
    "lr=0.01 #learning rate\n",
    "\n",
    "batch_size = 32\n",
    "val_split = .1 #trainset percentage allocated for devset\n",
    "test_val_split = .1 #trainset percentage allocated for test_val set (i.e. the test set of known patients)\n",
    "\n",
    "#cwd = os.getcwd()\n",
    "cwd = \"../subjects/min-max/windows_20/tr-True_sliding_20_c-False/folds_test\"\n",
    "subject = 1 # serve per caricare le folds da cartelle diverse\n",
    "prefix_train = 'TrainFold'\n",
    "prefix_test = 'TestFold'\n",
    "\n",
    "spw=20 #samples per window\n",
    "nmuscles=12 #initial number of muscles acquired\n",
    "\n",
    "#Enable/Disable shuffle on trainset/testset\n",
    "shuffle_train = False\n",
    "shuffle_test= False\n",
    "\n",
    "#Delete electrogonio signals\n",
    "exclude_features = False\n",
    "#Only use electrogonio signals\n",
    "include_only_features = False\n",
    "#Features to selected/deselected for input to the networks\n",
    "features_select = [1,12] #1 to 4\n",
    "\n",
    "#Select which models to run. Insert comma separated values into 'model_select' var.\n",
    "#List. 0:'FF', 1:'FC2', 2:'FC2DP', 3:'FC3', 4:'FC3dp', 5:'Conv1d', 6:'MultiConv1d' \n",
    "#e.g: model_select = [0,4,6] to select FF,FC3dp,MultiConv1d\n",
    "\n",
    "# Modelli del paper: 11 (FF2), 14 (FF4), 16 (FF5) --> Prova questi!\n",
    "# FF6 per testarlo potente dopo (17)\n",
    "model_lst = ['FF','FC2','FC2DP','FC3','FC3dp','Conv1d','MultiConv1d',\n",
    "             'MultiConv1d_2','MultiConv1d_3', 'MultiConv1d_4', 'MultiConv1d_5', \n",
    "             'FF2', 'CNN1', 'FF3', 'FF4', 'CNN2', 'FF5', 'FF6', 'CNN3', 'CNN1-FF5', 'CNN1-2','CNN1-1', 'CNN1-3', 'CNN_w60']\n",
    "model_select = [11] \n",
    "\n",
    "#Early stop settings\n",
    "maxepoch = 100\n",
    "maxpatience = 10\n",
    "\n",
    "use_cuda = False\n",
    "use_gputil = False\n",
    "cuda_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CUDA\n",
    "\n",
    "if use_gputil and torch.cuda.is_available():\n",
    "    import GPUtil\n",
    "\n",
    "    # Get the first available GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    try:\n",
    "        deviceIDs = GPUtil.getAvailable(order='memory', limit=1, maxLoad=100, maxMemory=20)  # return a list of available gpus\n",
    "    except:\n",
    "        print('GPU not compatible with NVIDIA-SMI')\n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(deviceIDs[0])\n",
    "        \n",
    "    ttens = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "    ttens = ttens.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? --> False\n",
      "Cuda Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print('Is CUDA available? --> ' + str(torch.cuda.is_available()))\n",
    "print('Cuda Device: ' + str(cuda_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seeds\n",
    "def setSeeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "setSeeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prints header of beautifultable report for each fold\n",
    "def header(model_list,nmodel,nfold,traindataset,testdataset):\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('MODEL: '+model_list[nmodel])\n",
    "    print('Fold: '+str(nfold))\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++\\n\\n')\n",
    "    shape = list(traindataset.x_data.shape)\n",
    "    print('Trainset fold'+str(i)+' shape: '+str(shape[0])+'x'+str((shape[1]+1)))\n",
    "    shape = list(testdataset.x_data.shape)\n",
    "    print('Testset fold'+str(i)+' shape: '+str(shape[0])+'x'+str((shape[1]+1))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prints actual beautifultable for each fold\n",
    "def table(model_list,nmodel,accuracies,precisions,recalls,f1_scores,accuracies_dev):\n",
    "    table = BeautifulTable()\n",
    "    table.column_headers = [\"{}\".format(model_list[nmodel]), \"Avg\", \"Stdev\"]\n",
    "    table.append_row([\"Accuracy\",round(np.average(accuracies),3),round(np.std(accuracies),3)])\n",
    "    table.append_row([\"Precision\",round(np.average(precisions),3),round(np.std(precisions),3)])\n",
    "    table.append_row([\"Recall\",round(np.average(recalls),3),round(np.std(recalls),3)])\n",
    "    table.append_row([\"F1_score\",round(np.average(f1_scores),3),round(np.std(f1_scores),3)])\n",
    "    table.append_row([\"Accuracy_dev\",round(np.average(accuracies_dev),3),round(np.std(accuracies_dev),3)])    \n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saves best model state on disk for each fold\n",
    "def save_checkpoint (state, is_best, filename, logfile):\n",
    "    if is_best:\n",
    "        msg = \"=> Saving a new best. \"+'Epoch: '+str(state['epoch'])\n",
    "        print (msg)\n",
    "        logfile.write(msg + \"\\n\")\n",
    "        torch.save(state, filename)  \n",
    "    else:\n",
    "        msg = \"=> Validation accuracy did not improve. \"+'Epoch: '+str(state['epoch'])\n",
    "        print (msg)\n",
    "        logfile.write(msg + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute sklearn metrics: Recall, Precision, F1-score\n",
    "def pre_rec (loader, model, positiveLabel):\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate (loader,0):\n",
    "            inputs, labels = data\n",
    "            y_true = np.append(y_true,labels.cpu())\n",
    "            outputs = model(inputs)\n",
    "            outputs[outputs>=0.5] = 1\n",
    "            outputs[outputs<0.5] = 0\n",
    "            y_pred = np.append(y_pred,outputs.cpu())\n",
    "    y_true = np.where(y_true==positiveLabel,0,1)\n",
    "    y_pred = np.where(y_pred==positiveLabel,0,1)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    return round(precision*100,3), round(recall*100,3), round(f1_score*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates model accuracy. Predicted vs Correct.\n",
    "def accuracy (loader, model):\n",
    "    total=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            outputs[outputs>=0.5] = 1\n",
    "            outputs[outputs<0.5] = 0\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "    return round((100 * correct / total),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Arrays to store metrics\n",
    "accs = np.empty([nfold,1])\n",
    "accs_test_val = np.empty([nfold,1])\n",
    "precisions_0_U = np.empty([nfold,1])\n",
    "recalls_0_U = np.empty([nfold,1])\n",
    "f1_scores_0_U = np.empty([nfold,1])\n",
    "precisions_1_U = np.empty([nfold,1])\n",
    "recalls_1_U = np.empty([nfold,1])\n",
    "f1_scores_1_U = np.empty([nfold,1])\n",
    "precisions_0_L = np.empty([nfold,1])\n",
    "recalls_0_L = np.empty([nfold,1])\n",
    "f1_scores_0_L = np.empty([nfold,1])\n",
    "precisions_1_L = np.empty([nfold,1])\n",
    "recalls_1_L = np.empty([nfold,1])\n",
    "f1_scores_1_L = np.empty([nfold,1])\n",
    "accs_dev = np.empty([nfold,1])\n",
    "times = np.empty([nfold,1])\n",
    "\n",
    "#Calculate avg metrics on folds\n",
    "def averages (vals):\n",
    "    avgs = []\n",
    "    for val in vals:\n",
    "        avgs.append(round(np.average(val),3))\n",
    "    return avgs\n",
    "\n",
    "#Calculate std metrics on folds\n",
    "def stds (vals):\n",
    "    stds = []\n",
    "    for val in vals:\n",
    "        stds.append(round(np.std(val),3))\n",
    "    return stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle\n",
    "def dev_shuffle (shuffle_train,shuffle_test,val_split,traindataset,testdataset):\n",
    "    train_size = len(traindataset)\n",
    "    test_size = len(testdataset)\n",
    "    train_indices = list(range(train_size))\n",
    "    test_indices = list(range(test_size))\n",
    "    split = int(np.floor(val_split * train_size))\n",
    "    if shuffle_train:\n",
    "        np.random.shuffle(train_indices)\n",
    "    if shuffle_test:\n",
    "        np.random.shuffle(test_indices) \n",
    "    train_indices, dev_indices = train_indices[split:], train_indices[:split]\n",
    "    # Samplers\n",
    "    tr_sampler = SubsetRandomSampler(train_indices)\n",
    "    d_sampler = SubsetRandomSampler(dev_indices)\n",
    "    te_sampler = SubsetRandomSampler(test_indices)\n",
    "    return tr_sampler,d_sampler,te_sampler\n",
    "\n",
    "def data_split (shuffle_train,shuffle_test,val_split,test_val_split,traindataset,testdataset):\n",
    "    train_size = len(traindataset)\n",
    "    test_size = len(testdataset)\n",
    "    train_indices = list(range(train_size))\n",
    "    test_indices = list(range(test_size))\n",
    "    test_val_split = int(np.floor(test_val_split * train_size)) \n",
    "    dev_split = int(np.floor(val_split * (train_size-test_val_split) + test_val_split))\n",
    "    if shuffle_train:\n",
    "        np.random.shuffle(train_indices)\n",
    "    if shuffle_test:\n",
    "        np.random.shuffle(test_indices) \n",
    "    train_indices, dev_indices, test_val_indices = train_indices[dev_split:], train_indices[test_val_split:dev_split], train_indices[:test_val_split]\n",
    "    # Samplers\n",
    "    tr_sampler = SubsetRandomSampler(train_indices)\n",
    "    d_sampler = SubsetRandomSampler(dev_indices)\n",
    "    tv_sampler = SubsetRandomSampler(test_val_indices)                \n",
    "    te_sampler = SubsetRandomSampler(test_indices)\n",
    "    return tr_sampler,d_sampler,tv_sampler,te_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 1\n",
      "Train dataset: 867073 Windows;\n",
      "Test dataset (LEARNED): 96389 Windows;\n",
      "Test dataset (UNLEARNED) 257150 Windows.\n",
      "Loading fold 2\n",
      "Train dataset: 895932 Windows;\n",
      "Test dataset (LEARNED): 99592 Windows;\n",
      "Test dataset (UNLEARNED) 225088 Windows.\n",
      "\n",
      "Each window is composed by 241 samples.\n",
      "The number of muscles is 12\n"
     ]
    }
   ],
   "source": [
    "#Loads and appends all folds all at once\n",
    "trainfolds = []    # Train set\n",
    "testfolds = []    # Test set (LEARNED)\n",
    "testfolds_U = []    # Test set (UNLEARNED)\n",
    "\n",
    "col_select = np.array([])\n",
    "\n",
    "#This is an hack to test smaller windows\n",
    "for i in range (spw*nmuscles,200):\n",
    "    col_select = np.append(col_select,i)\n",
    "    \n",
    "for i in range (0,spw*nmuscles,nmuscles):\n",
    "    for muscle in features_select:\n",
    "        col_select = np.append(col_select,muscle -1 + i)\n",
    "    cols=np.arange(0,spw*nmuscles+1)\n",
    "\n",
    "if exclude_features & (not include_only_features): #delete gonio\n",
    "    for j in range(fold_offset,fold_offset + nfold):\n",
    "        print(\"Loading fold \" + str(j))\n",
    "        traindata = pd.read_table(os.path.join(cwd, prefix_train + str(j)+'.csv'),sep=',',header=None,dtype=np.float32,usecols=[i for i in cols if i not in col_select.astype(int)])\n",
    "        trainfolds.append(traindata)\n",
    "        testdata = pd.read_table(os.path.join(cwd, prefix_test + str(j)+'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i not in col_select.astype(int)])\n",
    "        testfolds.append(testdata) \n",
    "elif include_only_features & (not exclude_features): #only gonio\n",
    "    for j in range(fold_offset, fold_offset + nfold):\n",
    "        print(\"Loading fold \" + str(j))\n",
    "        traindata = pd.read_table(os.path.join(cwd, prefix_train + str(j)+'.csv'),sep=',',header=None,dtype=np.float32,usecols=[i for i in cols if i in col_select.astype(int)])\n",
    "        testdata = pd.read_table(os.path.join(cwd, prefix_test + str(j)+'.csv'),sep=',',header=None,dtype=np.float32, usecols=[i for i in cols if i in col_select.astype(int)])\n",
    "        trainfolds.append(traindata)\n",
    "        testfolds.append(testdata) \n",
    "elif (not include_only_features) & (not exclude_features): \n",
    "    for j in range(fold_offset,fold_offset + nfold):\n",
    "        print(\"Loading fold \" + str(j))\n",
    "        traindata = pd.read_csv(os.path.join(cwd, prefix_train + '_' + str(j) + '.csv'),sep=',',header=None,dtype=np.float32)\n",
    "        testdata = pd.read_csv(os.path.join(cwd, prefix_test + '_L_' + str(j) +'.csv'),sep=',',header=None,dtype=np.float32)\n",
    "        testdata_U = pd.read_csv(os.path.join(cwd, prefix_test + '_U_' + str(j) +'.csv'),sep=',',header=None,dtype=np.float32)\n",
    "        trainfolds.append(traindata)\n",
    "        testfolds.append(testdata)\n",
    "        testfolds_U.append(testdata_U)    # Aggiunto testfold UNLEARNED \n",
    "        print('Train dataset: ' + str(len(traindata)) + ' windows;')\n",
    "        print('Test dataset (LEARNED): ' + str(len(testdata)) + ' windows;')\n",
    "        print('Test dataset (UNLEARNED) ' + str(len(testdata_U)) + ' windows.')\n",
    "else:\n",
    "    raise ValueError('use_gonio and del_gonio cannot be both True')\n",
    "\n",
    "print('\\nEach window is composed by ' + str(len(traindata.columns)) + ' samples.')\n",
    "print('The number of muscles is ' + str(nmuscles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmuscles=int((len(traindata.columns)-1)/spw) #used for layer dimensions and stride CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models\n",
    "from models import *\n",
    "models._spw = spw\n",
    "models._nmuscles = nmuscles\n",
    "models._batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(models._nmuscles)\n",
    "\n",
    "#import models\n",
    "#from models import *\n",
    "#TEST DIMENSIONS\n",
    "#models.nmuscles = nmuscles\n",
    "def testdimensions():\n",
    "    model = Model23()\n",
    "    print(model)\n",
    "    x = torch.randn(32,1,480)\n",
    "    model.test_dim(x)\n",
    " \n",
    "#testdimensions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fieldnames = ['Fold','Acc_L', 'Acc_U',\n",
    "              'R_0_U','R_1_U',\n",
    "              'R_0_L','R_1_L',\n",
    "              'Stop_epoch','Accuracy_dev'] #coloumn names report FOLD CSV\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "#TRAINING LOOP\n",
    "def train_test():\n",
    "    for k in model_select:\n",
    "        \n",
    "        table = BeautifulTable()\n",
    "        avgtable = BeautifulTable()\n",
    "        fieldnames1 = [model_lst[k],'Avg','Std_dev'] #column names report GLOBAL CSV\n",
    "        folder = os.path.join(cwd,'Report_'+str(model_lst[k]))\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "        logfilepath = os.path.join(folder,'log.txt')\n",
    "        logfile = open(logfilepath,\"w\") \n",
    "\n",
    "        with open(os.path.join(folder,'Report_folds.csv'),'w') as f_fold, open(os.path.join(folder,'Report_global.csv'),'w') as f_global:\n",
    "            writer = csv.DictWriter(f_fold, fieldnames = fieldnames)\n",
    "            writer1  = csv.DictWriter(f_global, fieldnames = fieldnames1)\n",
    "            writer.writeheader()\n",
    "            writer1.writeheader()\n",
    "            t0 = 0\n",
    "            t1 = 0\n",
    "            for i in range(1,nfold+1):\n",
    "                \n",
    "                t0 = time.time()\n",
    "                setSeeds(0)\n",
    "                \n",
    "                class Traindataset(Dataset):\n",
    "                    def __init__(self):\n",
    "                        self.data=trainfolds[i-1]\n",
    "                        self.x_data=torch.from_numpy(np.asarray(self.data.iloc[:, 0:-1])) \n",
    "                        self.len=self.data.shape[0]\n",
    "                        self.y_data = torch.from_numpy(np.asarray(self.data.iloc[:, [-1]]))\n",
    "                        if (use_cuda):\n",
    "                            self.x_data = self.x_data.cuda()\n",
    "                            self.y_data = self.y_data.cuda()\n",
    "                    def __getitem__(self, index):\n",
    "                        return self.x_data[index], self.y_data[index]\n",
    "                    def __len__(self):\n",
    "                        return self.len\n",
    "                class Testdataset(Dataset):\n",
    "                    def __init__(self):\n",
    "                        self.data=testfolds[i-1]\n",
    "                        self.x_data=torch.from_numpy(np.asarray(self.data.iloc[:, 0:-1]))\n",
    "                        self.len=self.data.shape[0]\n",
    "                        self.y_data = torch.from_numpy(np.asarray(self.data.iloc[:, [-1]]))\n",
    "                        if (use_cuda):\n",
    "                            self.x_data = self.x_data.cuda()\n",
    "                            self.y_data = self.y_data.cuda()\n",
    "                    def __getitem__(self, index):\n",
    "                        return self.x_data[index], self.y_data[index]\n",
    "                    def __len__(self):\n",
    "                        return self.len\n",
    "\n",
    "                traindataset = Traindataset()\n",
    "                testdataset = Testdataset()\n",
    "                testdataset_U = Testdataset()    # Aggiunto, relativo agli UNLEARNED\n",
    "\n",
    "                header(model_lst,k,i,traindataset,testdataset)\n",
    "\n",
    "                #train_sampler,dev_sampler,test_sampler=dev_shuffle(shuffle_train,shuffle_test,val_split,traindataset,testdataset)\n",
    "                train_sampler,dev_sampler,test_val_sampler,test_sampler=data_split(shuffle_train,shuffle_test,val_split,test_val_split,traindataset,testdataset)\n",
    "                \n",
    "                #loaders\n",
    "                train_loader = torch.utils.data.DataLoader(traindataset, batch_size=batch_size, \n",
    "                                                           sampler=train_sampler,drop_last=True)\n",
    "                test_val_loader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size,\n",
    "                                                                sampler=test_val_sampler,drop_last=True)\n",
    "                dev_loader = torch.utils.data.DataLoader(traindataset, batch_size=batch_size, \n",
    "                                                           sampler=dev_sampler,drop_last=True)\n",
    "                \n",
    "                # Questo l'ho cambiato da 'testdataset' a 'testdataset_U'\n",
    "                test_loader = torch.utils.data.DataLoader(testdataset_U, batch_size=batch_size,\n",
    "                                                                sampler=test_sampler,drop_last=True)\n",
    "                modelClass = \"Model\" + str(k)\n",
    "                model = eval(modelClass)()\n",
    "                \n",
    "                if (use_cuda):\n",
    "                    model = model.cuda()\n",
    "\n",
    "                if doTrain:\n",
    "                    \n",
    "                    criterion = nn.BCELoss(size_average=True)\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr)    \n",
    "                    msg = 'Accuracy on test set before training: '+str(accuracy(test_loader, model))+'\\n'\n",
    "                    print(msg)\n",
    "                    logfile.write(msg + \"\\n\")\n",
    "                    #EARLY STOP\n",
    "                    epoch = 0\n",
    "                    patience = 0\n",
    "                    best_acc_dev=0\n",
    "                    while (epoch<maxepoch and patience < maxpatience):\n",
    "                        running_loss = 0.0\n",
    "                        for l, data in enumerate(train_loader, 0):\n",
    "                            inputs, labels = data\n",
    "                            if use_cuda:\n",
    "                                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                            inputs, labels = Variable(inputs), Variable(labels)\n",
    "                            y_pred = model(inputs)\n",
    "                            if use_cuda:\n",
    "                                y_pred = y_pred.cuda()\n",
    "                            loss = criterion(y_pred, labels)\n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            running_loss += loss.item()\n",
    "                            #print accuracy ever l mini-batches\n",
    "                            if l % 2000 == 1999:\n",
    "                                msg = '[%d, %5d] loss: %.3f' %(epoch + 1, l + 1, running_loss / 999)\n",
    "                                print(msg)\n",
    "                                logfile.write(msg + \"\\n\")\n",
    "                                running_loss = 0.0\n",
    "                                #msg = 'Accuracy on dev set:' + str(accuracy(dev_loader))\n",
    "                                #print(msg)\n",
    "                                #logfile.write(msg + \"\\n\")        \n",
    "                        accdev = (accuracy(dev_loader, model))\n",
    "                        msg = 'Accuracy on dev set:' + str(accdev)\n",
    "                        print(msg)\n",
    "                        logfile.write(msg + \"\\n\")        \n",
    "                        is_best = bool(accdev > best_acc_dev)\n",
    "                        best_acc_dev = (max(accdev, best_acc_dev))\n",
    "                        save_checkpoint({\n",
    "                            'epoch': epoch + 1,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_acc_dev': best_acc_dev\n",
    "                        }, is_best,os.path.join(folder,'F'+str(i)+'best.pth.tar'), logfile)\n",
    "                        if is_best:\n",
    "                            patience=0\n",
    "                        else:\n",
    "                            patience = patience+1\n",
    "                        epoch = epoch+1\n",
    "                        logfile.flush()\n",
    "                        \n",
    "                if doEval:\n",
    "                    if use_cuda:                        \n",
    "                        state = torch.load(os.path.join(folder,'F'+str(i)+'best.pth.tar'))\n",
    "                    else:\n",
    "                        state = torch.load(os.path.join(folder,'F'+str(i)+'best.pth.tar'), map_location=lambda storage, loc: storage)\n",
    "                    stop_epoch = state['epoch']\n",
    "                    model.load_state_dict(state['state_dict'])\n",
    "                    if not use_cuda:\n",
    "                        model.cpu()\n",
    "                    accuracy_dev = state['best_acc_dev']\n",
    "                    model.eval()\n",
    "                    acctest = (accuracy(test_loader, model))\n",
    "                    acctest_val = (accuracy(test_val_loader, model))\n",
    "                    accs[i-1] = acctest\n",
    "                    accs_test_val[i-1] = acctest_val\n",
    "                    \n",
    "                    precision_0_U,recall_0_U,f1_score_0_U = pre_rec(test_loader, model, 0.0)\n",
    "                    precisions_0_U[i-1] = precision_0_U\n",
    "                    recalls_0_U[i-1] = recall_0_U\n",
    "                    f1_scores_0_U[i-1] = f1_score_0_U\n",
    "                    \n",
    "                    precision_1_U,recall_1_U,f1_score_1_U = pre_rec(test_loader, model, 1.0)\n",
    "                    precisions_1_U[i-1] = precision_1_U\n",
    "                    recalls_1_U[i-1] = recall_1_U\n",
    "                    f1_scores_1_U[i-1] = f1_score_1_U\n",
    "                    \n",
    "                    precision_0_L,recall_0_L,f1_score_0_L = pre_rec(test_val_loader, model, 0.0)\n",
    "                    precisions_0_L[i-1] = precision_0_L\n",
    "                    recalls_0_L[i-1] = recall_0_L\n",
    "                    f1_scores_0_L[i-1] = f1_score_0_L\n",
    "                    \n",
    "                    precision_1_L,recall_1_L,f1_score_1_L = pre_rec(test_val_loader, model, 1.0)\n",
    "                    precisions_1_L[i-1] = precision_1_L\n",
    "                    recalls_1_L[i-1] = recall_1_L\n",
    "                    f1_scores_1_L[i-1] = f1_score_1_L\n",
    "                    \n",
    "                    accs_dev[i-1] = accuracy_dev\n",
    "                    \n",
    "                    writer.writerow({'Fold': i,'Acc_L': acctest_val, 'Acc_U': acctest,\n",
    "                                     #'P_0_U': precision_0_U,'R_0_U': recall_0_U,'F1_0_U': f1_score_0_U,\n",
    "                                     'R_0_U': recall_0_U,\n",
    "                                     #'P_1_U': precision_1_U,'R_1_U': recall_1_U,'F1_1_U': f1_score_1_U,\n",
    "                                     'R_1_U': recall_1_U,\n",
    "                                     #'P_0_L': precision_0_L,'R_0_L': recall_0_L,'F1_0_L': f1_score_0_L,\n",
    "                                     'R_0_L': recall_0_L,\n",
    "                                     #'P_1_L': precision_1_L,'R_1_L': recall_1_L,'F1_1_L': f1_score_1_L,\n",
    "                                     'R_1_L': recall_1_L,\n",
    "                                     'Stop_epoch': stop_epoch,'Accuracy_dev': accuracy_dev})\n",
    "                    table.column_headers = fieldnames\n",
    "                    table.append_row([i,acctest_val,acctest,\n",
    "                                      #precision_0_U,recall_0_U,f1_score_0_U,\n",
    "                                      recall_0_U,\n",
    "                                      #precision_1_U,recall_1_U,f1_score_1_U,\n",
    "                                      recall_1_U,\n",
    "                                      #precision_0_L,recall_0_L,f1_score_0_L,\n",
    "                                      recall_0_L,\n",
    "                                      #precision_1_L,recall_1_L,f1_score_1_L,\n",
    "                                      recall_1_L,\n",
    "                                      stop_epoch,accuracy_dev])\n",
    "                    print(table)\n",
    "                    print('----------------------------------------------------------------------')\n",
    "                    logfile.write(str(table) + \"\\n----------------------------------------------------------------------\\n\")\n",
    "                    t1 = time.time()\n",
    "                    times[i-1] = int(t1-t0)\n",
    "            \n",
    "            duration = str(datetime.timedelta(seconds=np.sum(times)))\n",
    "            writer.writerow({})\n",
    "            writer.writerow({'Fold': 'Elapsed time: '+duration})\n",
    "            avg_acc_test_val = round(np.average(accs_test_val),3)\n",
    "            std_acc_test_val = round(np.std(accs_test_val),3)\n",
    "            \n",
    "            avg_acc_test_val,avg_a,avg_p_0_U,avg_r_0_U,avg_f_0_U,avg_p_1_U,avg_r_1_U,avg_f_1_U,avg_p_0_L,avg_r_0_L,avg_f_0_L,avg_p_1_L,avg_r_1_L,avg_f_1_L,avg_a_d=averages([accs_test_val,accs,precisions_0_U,recalls_0_U,f1_scores_0_U,precisions_1_U,recalls_1_U,f1_scores_1_U,precisions_0_L,recalls_0_L,f1_scores_0_L,precisions_1_L,recalls_1_L,f1_scores_1_L,accs_dev])\n",
    "            std_acc_test_val,std_a,std_p_0_U,std_r_0_U,std_f_0_U,std_p_1_U,std_r_1_U,std_f_1_U,std_p_0_L,std_r_0_L,std_f_0_L,std_p_1_L,std_r_1_L,std_f_1_L,std_a_d=stds([accs_test_val,accs,precisions_0_U,recalls_0_U,f1_scores_0_U,precisions_1_U,recalls_1_U,f1_scores_1_U,precisions_0_L,recalls_0_L,f1_scores_0_L,precisions_1_L,recalls_1_L,f1_scores_1_L,accs_dev])\n",
    "            \n",
    "            writer1.writerow({model_lst[k]: 'Acc_U','Avg': avg_a,'Std_dev': std_acc_test_val})\n",
    "            writer1.writerow({model_lst[k]: 'Acc_L','Avg': avg_acc_test_val,'Std_dev': std_a})\n",
    "            writer1.writerow({model_lst[k]: 'P_0_U','Avg': avg_p_0_U ,'Std_dev': std_p_0_U})\n",
    "            writer1.writerow({model_lst[k]: 'R_0_U','Avg': avg_r_0_U,'Std_dev': std_r_0_U})\n",
    "            writer1.writerow({model_lst[k]: 'F1_0_U','Avg': avg_f_0_U,'Std_dev': std_f_0_U})\n",
    "            writer1.writerow({model_lst[k]: 'P_1_U','Avg': avg_p_1_U,'Std_dev': std_p_1_U})\n",
    "            writer1.writerow({model_lst[k]: 'R_1_U','Avg': avg_r_1_U,'Std_dev': std_r_1_U})\n",
    "            writer1.writerow({model_lst[k]: 'F1_1_U','Avg': avg_f_1_U,'Std_dev': std_f_1_U})            \n",
    "            writer1.writerow({model_lst[k]: 'P_0_L','Avg': avg_p_0_L,'Std_dev': std_p_0_L})\n",
    "            writer1.writerow({model_lst[k]: 'R_0_L','Avg': avg_r_0_L,'Std_dev': std_r_0_L})\n",
    "            writer1.writerow({model_lst[k]: 'F1_0_L','Avg': avg_f_0_L,'Std_dev': std_f_0_L})\n",
    "            writer1.writerow({model_lst[k]: 'P_1_L','Avg': avg_p_1_L,'Std_dev': std_p_1_L})\n",
    "            writer1.writerow({model_lst[k]: 'R_1_L','Avg': avg_r_1_L,'Std_dev': std_r_1_L})\n",
    "            writer1.writerow({model_lst[k]: 'F1_1_L','Avg': avg_f_1_L,'Std_dev': std_f_1_L})                        \n",
    "            writer1.writerow({model_lst[k]: 'Acc_dev','Avg': avg_a_d,'Std_dev': std_a_d})\n",
    "            writer1.writerow({})\n",
    "            writer1.writerow({model_lst[k]: 'Elapsed time: '+duration})\n",
    "            avgtable.column_headers = fieldnames1\n",
    "            avgtable.append_row(['Acc_U',avg_a,std_a])\n",
    "            avgtable.append_row(['Acc_L',avg_acc_test_val,std_acc_test_val])\n",
    "            avgtable.append_row(['P_0_U',avg_p_0_U,std_p_0_U])\n",
    "            avgtable.append_row(['R_0_U',avg_r_0_U,std_r_0_U])\n",
    "            avgtable.append_row(['F1_0_U',avg_f_0_U,std_f_0_U])\n",
    "            avgtable.append_row(['P_1_U',avg_p_1_U,std_p_1_U])\n",
    "            avgtable.append_row(['R_1_U',avg_r_1_U,std_r_1_U])\n",
    "            avgtable.append_row(['F1_1_U',avg_f_1_U,std_f_1_U])                        \n",
    "            avgtable.append_row(['P_0_L',avg_p_0_L,std_p_0_L])\n",
    "            avgtable.append_row(['R_0_L',avg_r_0_L,std_r_0_L])\n",
    "            avgtable.append_row(['F1_0_L',avg_f_0_L,std_f_0_L])\n",
    "            avgtable.append_row(['P_1_L',avg_p_1_L,std_p_1_L])\n",
    "            avgtable.append_row(['R_1_L',avg_r_1_L,std_r_1_L])\n",
    "            avgtable.append_row(['F1_1_L',avg_f_1_L,std_f_1_L])            \n",
    "            avgtable.append_row(['Accuracy_dev',avg_a_d,std_a_d])\n",
    "            print(avgtable)\n",
    "            logfile.write(str(avgtable) + \"\\n\")\n",
    "            msg = 'Elapsed time: '+ duration + '\\n\\n'\n",
    "            print(msg)\n",
    "            logfile.write(msg )\n",
    "\n",
    "        logfile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am NOT using CUDA: SUCCESS!\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "MODEL: FF\n",
      "Fold: 1\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "Trainset fold228 shape: 867073x241\n",
      "Testset fold228 shape: 96389x241\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maskul/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set before training: 42.31\n",
      "\n",
      "[1,  2000] loss: 0.752\n",
      "[1,  4000] loss: 0.577\n",
      "[1,  6000] loss: 0.570\n",
      "[1,  8000] loss: 0.572\n",
      "[1, 10000] loss: 0.563\n",
      "[1, 12000] loss: 0.554\n",
      "[1, 14000] loss: 0.552\n",
      "[1, 16000] loss: 0.541\n",
      "[1, 18000] loss: 0.539\n",
      "[1, 20000] loss: 0.525\n",
      "Accuracy on dev set:91.084\n",
      "=> Saving a new best. Epoch: 1\n",
      "[2,  2000] loss: 0.515\n",
      "[2,  4000] loss: 0.503\n",
      "[2,  6000] loss: 0.499\n",
      "[2,  8000] loss: 0.494\n",
      "[2, 10000] loss: 0.489\n",
      "[2, 12000] loss: 0.485\n",
      "[2, 14000] loss: 0.478\n",
      "[2, 16000] loss: 0.476\n",
      "[2, 18000] loss: 0.475\n",
      "[2, 20000] loss: 0.466\n",
      "Accuracy on dev set:92.477\n",
      "=> Saving a new best. Epoch: 2\n",
      "[3,  2000] loss: 0.466\n",
      "[3,  4000] loss: 0.469\n",
      "[3,  6000] loss: 0.454\n",
      "[3,  8000] loss: 0.454\n",
      "[3, 10000] loss: 0.462\n",
      "[3, 12000] loss: 0.466\n",
      "[3, 14000] loss: 0.454\n",
      "[3, 16000] loss: 0.449\n",
      "[3, 18000] loss: 0.446\n",
      "[3, 20000] loss: 0.445\n",
      "Accuracy on dev set:93.139\n",
      "=> Saving a new best. Epoch: 3\n",
      "[4,  2000] loss: 0.439\n",
      "[4,  4000] loss: 0.443\n",
      "[4,  6000] loss: 0.443\n",
      "[4,  8000] loss: 0.442\n",
      "[4, 10000] loss: 0.439\n",
      "[4, 12000] loss: 0.437\n",
      "[4, 14000] loss: 0.441\n",
      "[4, 16000] loss: 0.435\n",
      "[4, 18000] loss: 0.428\n",
      "[4, 20000] loss: 0.439\n",
      "Accuracy on dev set:93.451\n",
      "=> Saving a new best. Epoch: 4\n",
      "[5,  2000] loss: 0.428\n",
      "[5,  4000] loss: 0.432\n",
      "[5,  6000] loss: 0.428\n",
      "[5,  8000] loss: 0.430\n",
      "[5, 10000] loss: 0.425\n",
      "[5, 12000] loss: 0.415\n",
      "[5, 14000] loss: 0.418\n",
      "[5, 16000] loss: 0.424\n",
      "[5, 18000] loss: 0.415\n",
      "[5, 20000] loss: 0.420\n",
      "Accuracy on dev set:93.858\n",
      "=> Saving a new best. Epoch: 5\n",
      "[6,  2000] loss: 0.413\n",
      "[6,  4000] loss: 0.412\n",
      "[6,  6000] loss: 0.421\n",
      "[6,  8000] loss: 0.423\n",
      "[6, 10000] loss: 0.416\n",
      "[6, 12000] loss: 0.413\n",
      "[6, 14000] loss: 0.414\n",
      "[6, 16000] loss: 0.409\n",
      "[6, 18000] loss: 0.409\n",
      "[6, 20000] loss: 0.412\n",
      "Accuracy on dev set:93.773\n",
      "=> Validation accuracy did not improve. Epoch: 6\n",
      "[7,  2000] loss: 0.411\n",
      "[7,  4000] loss: 0.407\n",
      "[7,  6000] loss: 0.408\n",
      "[7,  8000] loss: 0.403\n",
      "[7, 10000] loss: 0.403\n",
      "[7, 12000] loss: 0.401\n",
      "[7, 14000] loss: 0.407\n",
      "[7, 16000] loss: 0.404\n",
      "[7, 18000] loss: 0.403\n",
      "[7, 20000] loss: 0.400\n",
      "Accuracy on dev set:94.164\n",
      "=> Saving a new best. Epoch: 7\n",
      "[8,  2000] loss: 0.400\n",
      "[8,  4000] loss: 0.387\n",
      "[8,  6000] loss: 0.400\n",
      "[8,  8000] loss: 0.405\n",
      "[8, 10000] loss: 0.398\n",
      "[8, 12000] loss: 0.392\n",
      "[8, 14000] loss: 0.403\n",
      "[8, 16000] loss: 0.398\n",
      "[8, 18000] loss: 0.393\n",
      "[8, 20000] loss: 0.399\n",
      "Accuracy on dev set:93.99\n",
      "=> Validation accuracy did not improve. Epoch: 8\n",
      "[9,  2000] loss: 0.393\n",
      "[9,  4000] loss: 0.398\n",
      "[9,  6000] loss: 0.393\n",
      "[9,  8000] loss: 0.391\n",
      "[9, 10000] loss: 0.392\n",
      "[9, 12000] loss: 0.387\n",
      "[9, 14000] loss: 0.391\n",
      "[9, 16000] loss: 0.391\n",
      "[9, 18000] loss: 0.397\n",
      "[9, 20000] loss: 0.382\n",
      "Accuracy on dev set:93.955\n",
      "=> Validation accuracy did not improve. Epoch: 9\n",
      "[10,  2000] loss: 0.387\n",
      "[10,  4000] loss: 0.392\n",
      "[10,  6000] loss: 0.388\n",
      "[10,  8000] loss: 0.394\n",
      "[10, 10000] loss: 0.391\n",
      "[10, 12000] loss: 0.390\n",
      "[10, 14000] loss: 0.386\n",
      "[10, 16000] loss: 0.382\n",
      "[10, 18000] loss: 0.389\n",
      "[10, 20000] loss: 0.391\n",
      "Accuracy on dev set:94.01\n",
      "=> Validation accuracy did not improve. Epoch: 10\n",
      "[11,  2000] loss: 0.391\n",
      "[11,  4000] loss: 0.387\n",
      "[11,  6000] loss: 0.390\n",
      "[11,  8000] loss: 0.380\n",
      "[11, 10000] loss: 0.385\n",
      "[11, 12000] loss: 0.388\n",
      "[11, 14000] loss: 0.377\n",
      "[11, 16000] loss: 0.381\n",
      "[11, 18000] loss: 0.377\n",
      "[11, 20000] loss: 0.385\n",
      "Accuracy on dev set:93.931\n",
      "=> Validation accuracy did not improve. Epoch: 11\n",
      "[12,  2000] loss: 0.385\n",
      "[12,  4000] loss: 0.376\n",
      "[12,  6000] loss: 0.384\n",
      "[12,  8000] loss: 0.384\n",
      "[12, 10000] loss: 0.381\n",
      "[12, 12000] loss: 0.380\n",
      "[12, 14000] loss: 0.378\n",
      "[12, 16000] loss: 0.382\n",
      "[12, 18000] loss: 0.381\n",
      "[12, 20000] loss: 0.378\n",
      "Accuracy on dev set:94.299\n",
      "=> Saving a new best. Epoch: 12\n",
      "[13,  2000] loss: 0.378\n",
      "[13,  4000] loss: 0.386\n",
      "[13,  6000] loss: 0.377\n",
      "[13,  8000] loss: 0.382\n",
      "[13, 10000] loss: 0.371\n",
      "[13, 12000] loss: 0.378\n",
      "[13, 14000] loss: 0.374\n",
      "[13, 16000] loss: 0.382\n",
      "[13, 18000] loss: 0.380\n",
      "[13, 20000] loss: 0.380\n",
      "Accuracy on dev set:93.759\n",
      "=> Validation accuracy did not improve. Epoch: 13\n",
      "[14,  2000] loss: 0.380\n",
      "[14,  4000] loss: 0.372\n",
      "[14,  6000] loss: 0.384\n",
      "[14,  8000] loss: 0.381\n",
      "[14, 10000] loss: 0.369\n",
      "[14, 12000] loss: 0.374\n",
      "[14, 14000] loss: 0.374\n",
      "[14, 16000] loss: 0.375\n",
      "[14, 18000] loss: 0.383\n",
      "[14, 20000] loss: 0.370\n",
      "Accuracy on dev set:94.35\n",
      "=> Saving a new best. Epoch: 14\n",
      "[15,  2000] loss: 0.372\n",
      "[15,  4000] loss: 0.376\n",
      "[15,  6000] loss: 0.375\n",
      "[15,  8000] loss: 0.373\n",
      "[15, 10000] loss: 0.370\n",
      "[15, 12000] loss: 0.369\n",
      "[15, 14000] loss: 0.383\n",
      "[15, 16000] loss: 0.374\n",
      "[15, 18000] loss: 0.375\n",
      "[15, 20000] loss: 0.379\n",
      "Accuracy on dev set:93.805\n",
      "=> Validation accuracy did not improve. Epoch: 15\n",
      "[16,  2000] loss: 0.374\n",
      "[16,  4000] loss: 0.381\n",
      "[16,  6000] loss: 0.369\n",
      "[16,  8000] loss: 0.374\n",
      "[16, 10000] loss: 0.374\n",
      "[16, 12000] loss: 0.374\n",
      "[16, 14000] loss: 0.372\n",
      "[16, 16000] loss: 0.367\n",
      "[16, 18000] loss: 0.367\n",
      "[16, 20000] loss: 0.373\n",
      "Accuracy on dev set:94.563\n",
      "=> Saving a new best. Epoch: 16\n",
      "[17,  2000] loss: 0.375\n",
      "[17,  4000] loss: 0.370\n",
      "[17,  6000] loss: 0.363\n",
      "[17,  8000] loss: 0.369\n",
      "[17, 10000] loss: 0.373\n",
      "[17, 12000] loss: 0.376\n",
      "[17, 14000] loss: 0.376\n",
      "[17, 16000] loss: 0.374\n",
      "[17, 18000] loss: 0.364\n",
      "[17, 20000] loss: 0.371\n",
      "Accuracy on dev set:94.64\n",
      "=> Saving a new best. Epoch: 17\n",
      "[18,  2000] loss: 0.372\n",
      "[18,  4000] loss: 0.373\n",
      "[18,  6000] loss: 0.368\n",
      "[18,  8000] loss: 0.367\n",
      "[18, 10000] loss: 0.369\n",
      "[18, 12000] loss: 0.372\n",
      "[18, 14000] loss: 0.371\n",
      "[18, 16000] loss: 0.366\n",
      "[18, 18000] loss: 0.369\n",
      "[18, 20000] loss: 0.361\n",
      "Accuracy on dev set:94.566\n",
      "=> Validation accuracy did not improve. Epoch: 18\n",
      "[19,  2000] loss: 0.377\n",
      "[19,  4000] loss: 0.373\n",
      "[19,  6000] loss: 0.363\n",
      "[19,  8000] loss: 0.372\n",
      "[19, 10000] loss: 0.366\n",
      "[19, 12000] loss: 0.366\n",
      "[19, 14000] loss: 0.363\n",
      "[19, 16000] loss: 0.368\n",
      "[19, 18000] loss: 0.368\n",
      "[19, 20000] loss: 0.364\n",
      "Accuracy on dev set:94.392\n",
      "=> Validation accuracy did not improve. Epoch: 19\n",
      "[20,  2000] loss: 0.369\n",
      "[20,  4000] loss: 0.369\n",
      "[20,  6000] loss: 0.367\n",
      "[20,  8000] loss: 0.361\n",
      "[20, 10000] loss: 0.360\n",
      "[20, 12000] loss: 0.367\n",
      "[20, 14000] loss: 0.365\n",
      "[20, 16000] loss: 0.366\n",
      "[20, 18000] loss: 0.369\n",
      "[20, 20000] loss: 0.366\n",
      "Accuracy on dev set:94.691\n",
      "=> Saving a new best. Epoch: 20\n",
      "[21,  2000] loss: 0.362\n",
      "[21,  4000] loss: 0.364\n",
      "[21,  6000] loss: 0.365\n",
      "[21,  8000] loss: 0.362\n",
      "[21, 10000] loss: 0.364\n",
      "[21, 12000] loss: 0.370\n",
      "[21, 14000] loss: 0.366\n",
      "[21, 16000] loss: 0.370\n",
      "[21, 18000] loss: 0.370\n",
      "[21, 20000] loss: 0.364\n",
      "Accuracy on dev set:94.145\n",
      "=> Validation accuracy did not improve. Epoch: 21\n",
      "[22,  2000] loss: 0.370\n",
      "[22,  4000] loss: 0.362\n",
      "[22,  6000] loss: 0.364\n",
      "[22,  8000] loss: 0.366\n",
      "[22, 10000] loss: 0.365\n",
      "[22, 12000] loss: 0.362\n",
      "[22, 14000] loss: 0.365\n",
      "[22, 16000] loss: 0.360\n",
      "[22, 18000] loss: 0.364\n",
      "[22, 20000] loss: 0.362\n",
      "Accuracy on dev set:94.265\n",
      "=> Validation accuracy did not improve. Epoch: 22\n",
      "[23,  2000] loss: 0.364\n",
      "[23,  4000] loss: 0.364\n",
      "[23,  6000] loss: 0.362\n",
      "[23,  8000] loss: 0.364\n",
      "[23, 10000] loss: 0.361\n",
      "[23, 12000] loss: 0.359\n",
      "[23, 14000] loss: 0.367\n",
      "[23, 16000] loss: 0.373\n",
      "[23, 18000] loss: 0.361\n",
      "[23, 20000] loss: 0.360\n",
      "Accuracy on dev set:94.454\n",
      "=> Validation accuracy did not improve. Epoch: 23\n",
      "[24,  2000] loss: 0.361\n",
      "[24,  4000] loss: 0.359\n",
      "[24,  6000] loss: 0.357\n",
      "[24,  8000] loss: 0.361\n",
      "[24, 10000] loss: 0.362\n",
      "[24, 12000] loss: 0.367\n",
      "[24, 14000] loss: 0.364\n",
      "[24, 16000] loss: 0.364\n",
      "[24, 18000] loss: 0.360\n",
      "[24, 20000] loss: 0.358\n",
      "Accuracy on dev set:94.665\n",
      "=> Validation accuracy did not improve. Epoch: 24\n",
      "[25,  2000] loss: 0.360\n",
      "[25,  4000] loss: 0.354\n",
      "[25,  6000] loss: 0.363\n",
      "[25,  8000] loss: 0.354\n",
      "[25, 10000] loss: 0.366\n",
      "[25, 12000] loss: 0.358\n",
      "[25, 14000] loss: 0.360\n",
      "[25, 16000] loss: 0.365\n",
      "[25, 18000] loss: 0.364\n",
      "[25, 20000] loss: 0.359\n",
      "Accuracy on dev set:94.381\n",
      "=> Validation accuracy did not improve. Epoch: 25\n",
      "[26,  2000] loss: 0.354\n",
      "[26,  4000] loss: 0.349\n",
      "[26,  6000] loss: 0.354\n",
      "[26,  8000] loss: 0.373\n",
      "[26, 10000] loss: 0.364\n",
      "[26, 12000] loss: 0.354\n",
      "[26, 14000] loss: 0.367\n",
      "[26, 16000] loss: 0.355\n",
      "[26, 18000] loss: 0.357\n",
      "[26, 20000] loss: 0.366\n",
      "Accuracy on dev set:94.023\n",
      "=> Validation accuracy did not improve. Epoch: 26\n",
      "[27,  2000] loss: 0.359\n",
      "[27,  4000] loss: 0.355\n",
      "[27,  6000] loss: 0.361\n",
      "[27,  8000] loss: 0.359\n",
      "[27, 10000] loss: 0.360\n",
      "[27, 12000] loss: 0.354\n",
      "[27, 14000] loss: 0.357\n",
      "[27, 16000] loss: 0.363\n",
      "[27, 18000] loss: 0.363\n",
      "[27, 20000] loss: 0.357\n",
      "Accuracy on dev set:94.383\n",
      "=> Validation accuracy did not improve. Epoch: 27\n",
      "[28,  2000] loss: 0.361\n",
      "[28,  4000] loss: 0.357\n",
      "[28,  6000] loss: 0.356\n",
      "[28,  8000] loss: 0.364\n",
      "[28, 10000] loss: 0.354\n",
      "[28, 12000] loss: 0.356\n",
      "[28, 14000] loss: 0.358\n",
      "[28, 16000] loss: 0.357\n",
      "[28, 18000] loss: 0.360\n",
      "[28, 20000] loss: 0.359\n",
      "Accuracy on dev set:94.074\n",
      "=> Validation accuracy did not improve. Epoch: 28\n",
      "[29,  2000] loss: 0.355\n",
      "[29,  4000] loss: 0.359\n",
      "[29,  6000] loss: 0.353\n",
      "[29,  8000] loss: 0.352\n",
      "[29, 10000] loss: 0.356\n",
      "[29, 12000] loss: 0.357\n",
      "[29, 14000] loss: 0.370\n",
      "[29, 16000] loss: 0.357\n",
      "[29, 18000] loss: 0.358\n",
      "[29, 20000] loss: 0.358\n",
      "Accuracy on dev set:94.354\n",
      "=> Validation accuracy did not improve. Epoch: 29\n",
      "[30,  2000] loss: 0.359\n",
      "[30,  4000] loss: 0.359\n",
      "[30,  6000] loss: 0.349\n",
      "[30,  8000] loss: 0.357\n",
      "[30, 10000] loss: 0.355\n",
      "[30, 12000] loss: 0.363\n",
      "[30, 14000] loss: 0.346\n",
      "[30, 16000] loss: 0.354\n",
      "[30, 18000] loss: 0.361\n",
      "[30, 20000] loss: 0.361\n",
      "Accuracy on dev set:94.451\n",
      "=> Validation accuracy did not improve. Epoch: 30\n",
      "[31,  2000] loss: 0.352\n",
      "[31,  4000] loss: 0.358\n",
      "[31,  6000] loss: 0.355\n",
      "[31,  8000] loss: 0.352\n",
      "[31, 10000] loss: 0.354\n",
      "[31, 12000] loss: 0.361\n",
      "[31, 14000] loss: 0.352\n",
      "[31, 16000] loss: 0.351\n",
      "[31, 18000] loss: 0.360\n",
      "[31, 20000] loss: 0.359\n",
      "Accuracy on dev set:94.233\n",
      "=> Validation accuracy did not improve. Epoch: 31\n",
      "[32,  2000] loss: 0.354\n",
      "[32,  4000] loss: 0.358\n",
      "[32,  6000] loss: 0.354\n",
      "[32,  8000] loss: 0.353\n",
      "[32, 10000] loss: 0.354\n",
      "[32, 12000] loss: 0.360\n",
      "[32, 14000] loss: 0.352\n",
      "[32, 16000] loss: 0.359\n",
      "[32, 18000] loss: 0.358\n",
      "[32, 20000] loss: 0.351\n",
      "Accuracy on dev set:94.552\n",
      "=> Validation accuracy did not improve. Epoch: 32\n",
      "[33,  2000] loss: 0.351\n",
      "[33,  4000] loss: 0.352\n",
      "[33,  6000] loss: 0.356\n",
      "[33,  8000] loss: 0.362\n",
      "[33, 10000] loss: 0.355\n",
      "[33, 12000] loss: 0.355\n",
      "[33, 14000] loss: 0.351\n",
      "[33, 16000] loss: 0.352\n",
      "[33, 18000] loss: 0.353\n",
      "[33, 20000] loss: 0.359\n",
      "Accuracy on dev set:94.338\n",
      "=> Validation accuracy did not improve. Epoch: 33\n",
      "[34,  2000] loss: 0.343\n",
      "[34,  4000] loss: 0.357\n",
      "[34,  6000] loss: 0.355\n",
      "[34,  8000] loss: 0.365\n",
      "[34, 10000] loss: 0.358\n",
      "[34, 12000] loss: 0.359\n",
      "[34, 14000] loss: 0.352\n",
      "[34, 16000] loss: 0.352\n",
      "[34, 18000] loss: 0.354\n",
      "[34, 20000] loss: 0.349\n",
      "Accuracy on dev set:94.572\n",
      "=> Validation accuracy did not improve. Epoch: 34\n",
      "[35,  2000] loss: 0.357\n",
      "[35,  4000] loss: 0.357\n",
      "[35,  6000] loss: 0.354\n",
      "[35,  8000] loss: 0.349\n",
      "[35, 10000] loss: 0.361\n",
      "[35, 12000] loss: 0.351\n",
      "[35, 14000] loss: 0.356\n",
      "[35, 16000] loss: 0.350\n",
      "[35, 18000] loss: 0.355\n",
      "[35, 20000] loss: 0.352\n",
      "Accuracy on dev set:94.331\n",
      "=> Validation accuracy did not improve. Epoch: 35\n",
      "[36,  2000] loss: 0.353\n",
      "[36,  4000] loss: 0.353\n",
      "[36,  6000] loss: 0.361\n",
      "[36,  8000] loss: 0.354\n",
      "[36, 10000] loss: 0.355\n",
      "[36, 12000] loss: 0.362\n",
      "[36, 14000] loss: 0.348\n",
      "[36, 16000] loss: 0.347\n",
      "[36, 18000] loss: 0.351\n",
      "[36, 20000] loss: 0.349\n",
      "Accuracy on dev set:93.574\n",
      "=> Validation accuracy did not improve. Epoch: 36\n",
      "[37,  2000] loss: 0.349\n",
      "[37,  4000] loss: 0.354\n",
      "[37,  6000] loss: 0.354\n",
      "[37,  8000] loss: 0.351\n",
      "[37, 10000] loss: 0.354\n",
      "[37, 12000] loss: 0.351\n",
      "[37, 14000] loss: 0.353\n",
      "[37, 16000] loss: 0.355\n",
      "[37, 18000] loss: 0.355\n",
      "[37, 20000] loss: 0.352\n",
      "Accuracy on dev set:94.482\n",
      "=> Validation accuracy did not improve. Epoch: 37\n",
      "[38,  2000] loss: 0.357\n",
      "[38,  4000] loss: 0.349\n",
      "[38,  6000] loss: 0.348\n",
      "[38,  8000] loss: 0.353\n",
      "[38, 10000] loss: 0.350\n",
      "[38, 12000] loss: 0.341\n",
      "[38, 14000] loss: 0.355\n",
      "[38, 16000] loss: 0.348\n",
      "[38, 18000] loss: 0.355\n",
      "[38, 20000] loss: 0.365\n",
      "Accuracy on dev set:94.631\n",
      "=> Validation accuracy did not improve. Epoch: 38\n",
      "[39,  2000] loss: 0.358\n",
      "[39,  4000] loss: 0.357\n",
      "[39,  6000] loss: 0.353\n",
      "[39,  8000] loss: 0.357\n",
      "[39, 10000] loss: 0.347\n",
      "[39, 12000] loss: 0.353\n",
      "[39, 14000] loss: 0.352\n",
      "[39, 16000] loss: 0.346\n",
      "[39, 18000] loss: 0.347\n",
      "[39, 20000] loss: 0.348\n",
      "Accuracy on dev set:94.1\n",
      "=> Validation accuracy did not improve. Epoch: 39\n",
      "[40,  2000] loss: 0.352\n",
      "[40,  4000] loss: 0.349\n",
      "[40,  6000] loss: 0.347\n",
      "[40,  8000] loss: 0.355\n",
      "[40, 10000] loss: 0.343\n",
      "[40, 12000] loss: 0.352\n",
      "[40, 14000] loss: 0.350\n",
      "[40, 16000] loss: 0.352\n",
      "[40, 18000] loss: 0.352\n",
      "[40, 20000] loss: 0.354\n",
      "Accuracy on dev set:94.17\n",
      "=> Validation accuracy did not improve. Epoch: 40\n",
      "+------+-------+-------+-------+-------+-------+-------+----------+------------+\n",
      "| Fold | Acc_L | Acc_U | R_0_U | R_1_U | R_0_L | R_1_L | Stop_epo | Accuracy_d |\n",
      "|      |       |       |       |       |       |       |    ch    |     ev     |\n",
      "+------+-------+-------+-------+-------+-------+-------+----------+------------+\n",
      "|  1   | 90.82 | 90.70 | 87.93 | 92.66 | 88.00 | 92.79 |    20    |   94.691   |\n",
      "|      |   3   |   6   |   8   |   5   |   9   |   6   |          |            |\n",
      "+------+-------+-------+-------+-------+-------+-------+----------+------------+\n",
      "----------------------------------------------------------------------\n",
      "+--------------+--------+---------+\n",
      "|      FF      |  Avg   | Std_dev |\n",
      "+--------------+--------+---------+\n",
      "|    Acc_U     | 90.706 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    Acc_L     | 90.823 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    P_0_U     | 89.456 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    R_0_U     | 87.938 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    F1_0_U    | 88.69  |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    P_1_U     | 91.567 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    R_1_U     | 92.665 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    F1_1_U    | 92.113 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    P_0_L     | 89.54  |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    R_0_L     | 88.009 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    F1_0_L    | 88.768 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    P_1_L     |  91.7  |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    R_1_L     | 92.796 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "|    F1_1_L    | 92.245 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "| Accuracy_dev | 94.691 |   0.0   |\n",
      "+--------------+--------+---------+\n",
      "Elapsed time: 0:23:54\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmuscles=int((len(traindata.columns)-1)/spw)\n",
    "if use_cuda and not use_gputil and cuda_device!=None and torch.cuda.is_available():\n",
    "    with torch.cuda.device(cuda_device):\n",
    "        print('I am using CUDA: SUCCESS!')\n",
    "        train_test()\n",
    "else:\n",
    "    print('I am NOT using CUDA: SUCCESS!')\n",
    "    train_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
